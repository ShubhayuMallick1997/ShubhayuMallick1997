# 2 . **HDFS (Hadoop Distributed File System)**

## **HDFS Architecture**

Hadoop Distributed File System (HDFS) is the storage layer of Hadoop and is designed to store large files across multiple machines in a distributed manner. The architecture of HDFS is optimized for high throughput and is fault-tolerant, meaning it ensures data is replicated across multiple nodes, making it resilient to node failures.

HDFS consists of two primary components:

1. **NameNode (Master Node)**:

   * The NameNode is the **master** of the HDFS system. It stores and manages the metadata of the file system.
   * **Metadata Management**: It contains information about file names, directories, file permissions, and the locations of data blocks stored across the cluster.
   * The NameNode does not store the actual data; it only keeps track of where the data is located.
   * It is the most critical component of HDFS and is responsible for managing the file system namespace, determining how files are split into blocks, and tracking block locations.
   * **Fault Tolerance**: In case of a NameNode failure, the entire file system becomes inaccessible. To mitigate this, a **Secondary NameNode** is used for periodic checkpoints of the NameNode's state.

2. **DataNode (Worker Node)**:

   * DataNodes are the **worker nodes** in HDFS that are responsible for storing the actual data blocks.
   * They periodically report to the NameNode with a **block report** that indicates which blocks are stored on which DataNode.
   * DataNodes handle read and write requests from clients. They also replicate blocks based on the instructions from the NameNode to ensure fault tolerance.

3. **Block**:

   * HDFS splits large files into smaller chunks called **blocks** (typically 128MB or 256MB in size). These blocks are distributed across the DataNodes.
   * This block-based approach allows HDFS to store large files efficiently and process them in parallel across the cluster.

4. **Heartbeat and Block Report**:

   * **Heartbeat**: DataNodes send periodic heartbeats to the NameNode to inform it that they are still alive. If the NameNode stops receiving a heartbeat from a DataNode, it considers that node as failed.
   * **Block Report**: Each DataNode sends a report to the NameNode that lists all the blocks it is storing. This report is used for block replication and to keep track of data availability.

## **Data Replication**

One of the key features of HDFS is **data replication**. By default, HDFS replicates each block of data three times across different nodes. The number of replicas can be configured according to the requirements of the system.

* **Replica Placement**: When data is written to HDFS, the blocks are replicated in a way that ensures data availability and fault tolerance. The replicas are placed on different DataNodes across the cluster, and the replicas are spread across racks to prevent data loss due to rack failures.
* **Fault Tolerance**: If one DataNode fails, HDFS ensures that the replica of the block is still available from another DataNode. Additionally, HDFS automatically detects block loss and initiates the replication of missing blocks to maintain the configured replication factor.

## **HDFS Operations**

HDFS supports various file operations similar to any standard file system, such as:

1. **Read Operation**:

   * When a client wants to read a file, the NameNode provides the location of the blocks storing that file.
   * The client can directly read the blocks from the DataNodes.
   * Since blocks are replicated, the client can read from any replica.

2. **Write Operation**:

   * When a client wants to write data to HDFS, it first contacts the NameNode to get a list of DataNodes where the blocks should be written.
   * The data is then written to the DataNodes in blocks, and each block is replicated based on the replication factor.
   * Once all blocks are written, the metadata is updated in the NameNode.

3. **File Deletion**:

   * When a file is deleted, the client sends the request to the NameNode.
   * The NameNode updates the metadata and removes the reference to the file. The actual data blocks are deleted from the DataNodes.

4. **File Renaming**:

   * File renaming is done by deleting the old entry and adding a new one to the metadata in the NameNode.
   * The actual blocks of data are not moved in the cluster; only the reference is updated in the NameNode.

## **Fault Tolerance and Data Integrity**

HDFS is designed to handle hardware failures, which are common in large distributed systems. Here’s how HDFS ensures **fault tolerance** and **data integrity**:

1. **Block Replication**:

   * Each block is replicated multiple times across the cluster. If one replica becomes unavailable due to node failure, the remaining replicas can be used to ensure data availability.

2. **Data Integrity**:

   * HDFS uses a checksum to ensure data integrity. When data is written to the system, a checksum is calculated and stored alongside the data. When the data is read, the checksum is recalculated to verify that the data hasn’t been corrupted.

3. **Automatic Recovery**:

   * If a block replica is lost, HDFS automatically detects this and creates a new replica to maintain the desired replication factor.
   * The system also detects when DataNodes are failing (through heartbeats), and once a failure is detected, HDFS will automatically re-replicate blocks from the failed node to ensure data availability.

## **HDFS Commands**

HDFS provides a command-line interface (CLI) for managing files and directories in the file system. Some common HDFS commands include:

1. **Creating a Directory**: `hdfs dfs -mkdir /user/hadoop/data`
2. **Copying Files**: `hdfs dfs -copyFromLocal localfile.txt /user/hadoop/data/`
3. **Listing Files**: `hdfs dfs -ls /user/hadoop/`
4. **Checking File Status**: `hdfs dfs -stat /user/hadoop/data/file.txt`
5. **Deleting Files**: `hdfs dfs -rm /user/hadoop/data/file.txt`
6. **Viewing File Content**: `hdfs dfs -cat /user/hadoop/data/file.txt`

## **HDFS Client API**

HDFS also provides a Java-based API that allows clients to interact with the file system programmatically. The basic operations available through the API include:

* **File Creation**: To create a file in HDFS.
* **File Reading**: To read files from HDFS.
* **File Deletion**: To delete files from HDFS.
* **Listing Files**: To list files and directories in HDFS.

## **Conclusion**

HDFS is the backbone of the Hadoop ecosystem and plays a critical role in storing large datasets across a distributed cluster. By providing fault tolerance through block replication, ensuring high throughput through parallel storage, and being scalable to accommodate increasing data volumes, HDFS allows Hadoop to handle big data effectively. The combination of HDFS with other Hadoop components like MapReduce, YARN, and Hive enables the efficient processing and analysis of data in a distributed computing environment.
